# 第2章 感知机

感知机 perceptron 是二分类的线性模型，输入为**实例的特征向量**，输出为**类别1或-1**,也就是

> 对应于输入空间（特征空间）将实例划分为正负两类的分离超平面，属于**判别模型**

损失函数是基于误分类（？），利用**梯度下降法**对损失函数进行极小化，求得感知机模型

学习算法分为：

- 原始形式
- 对偶形式

重要性： 神经网络与支持向量机的基础

1957年由Rosenblatt提出



## 2.1 感知机模型

输入x表示实例的特征向量，输出y表示类别，则

> f(x) = sign(w*x + b)

称为感知机，其中w，b为参数， w为权重向量weight vector, b成为偏置bias，w*x就是正常向量的內积，sign是符号函数。

感知机模型的假设空间是

> 定义在特征空间中的所有线性分类模型linear classification model 或者线性分类器linear classifier。

感知机的几何解释：

> 线性方程 w * x + b = 0 对应于特征空间的超平面S

其中 w是S的法向量，b是S的截距，S将特征空间分为两部分，因此称为分离超平面 separating hyperplane.



例子：二维坐标系中的直线，ax + by + c = 0：

- 特征空间是二维平面的点
- 分离超平面是一条直线
- w = (a,b), b = c
- 直线将二维平面分为两部分，直线上面为正，下面为负



感知机学习的过程，就是求得感知机模型中的参数w, b 的过程。



## 2.2 感知机学习策略

###  2.2.1 数据集的线性可分性

对于给定数据集，如果存在某个超平面，可以将数据集的正、负实例点完全正确的划分到超平面的两侧，则称该数据集是线性可分的。



### 2.2.2 感知机学习策略

如果训练数据是线性可分的，那么感知机的学习目标就是找分离超平面 separating hyperplane，也就是确定w,b!

确定学习策略：

> 定义损失函数并最小化

分析：

> 损失函数采用误分类的点，但是这样关于w,b 不是连续可导函数，优化起来很难

感知机的损失函数：

> 误分类点到超平面的总距离

关于输入空间上任意一点到超平面上的距离定义，与二维空间中点到直线，与三维空间中点到平面的定义一样，即

> | w * x + b | / |w|



对于误分类的点有如下性质：

> -y * ( w * x + b ) > 0  , 用来去掉距离中的绝对值符号

然后误分类点到超平面S的距离可以写成

> -y * (w * x + b ) / |w|

然后就可以得出感知机的损失函数

> 所有误分类点到超平面距离的和（如何确定哪些是误分类的呢？？）

