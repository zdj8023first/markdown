# 第三章  k近邻法

k近邻法 k-nearest neighbor , k-NN 是基本的分类和回归的算法，作者只介绍分类。

> - 输入为实例的特征向量，对应特征空间中的点
> - 输出为类别，可以为多类别
> - 分类时，通过多数表决等进行预测
> - 三基本要素：k值的选择、距离度量、分类决策规则
> - kd树以及构造算法



## 3.1 k近邻算法

算法过程简述：

> 给定训练数据集， 对于新的输入实例，在训练数据集中找到k个与它最邻近的k个实例，这k个中多数属于的类别，就是该输入实例的类别。

当k =1时，称为最近邻算法。

**k近邻算法没有显示的学习过程**



## 3.2 k近邻模型

对应与对特征空间的划分。

三要素： 

- 距离度量
- k值选择
- 分类决策规则



### 3.2.1 模型

当k近邻算法的三要素确定之后，对于新的实例，它所属的类也唯一确定。

三要素的确定相当于将特征空间划分为子空间，确定空间中的点所属的类。

例子：最近邻算法

最近邻算法中每个训练点的周围有一个区域，这个区域中点的最近点都是该训练点，称为单元cell，该训练点就称为该cell的类标记 class label。划分的例子见原书图3.1.

> 最近邻划分的线为相邻两点连线的垂直平分线。



### 3.2.2 距离度量

特征空间中两个点的距离是

> 两个实例点的相似程度的反应

一般有：

- 欧氏距离
- Lp距离 Lp distance
- Minkowski distance

Lp distance 的定义：

对于两个特征向量，它们之间的Lp distance的定义 如下：

> Lp(x1, x2) = x1和x2对应分量的差的绝对值的p次方的和，然后对和开p次平方

当p=2时，称为欧氏距离 Euclidean distance；

当p=1时，称为曼哈顿距离 Manhattan distance；

当p为无穷大时，求个极限，值为各个分量距离的最大值。

具体图形化可见原书 P3.2

### 3.2.3	k值的选择

- 当k较小时，近似误差approximation error减小，估计误差estimation error增大，模型复杂，易发生过拟合。例子：当k =1时，即最近邻
- 当k较大时，近似误差增大，估计误差减小，模型简单。当k=N时，N为训练数据集大小，不管输入什么实例变量，都会预测为训练数据中的最多的类。

应用：k一般取 较小的值，并通过交叉验证来选取最优的k值



### 3.2.4 分类决策规则

k近邻法中往往采用多数表决的决策规则。而多数表决 majority voting rule保证了**经验风险最小化**，具体证明过程详见原书第40页。



## 3.3 k近邻法的实现： kd树 k dimension tree

实现的主要问题在于：

> 如何对训练数据进行快速的k近邻搜索。

为了提高效率，使用特殊的数据结构存储训练数据，以减少计算距离的次数。	



### 3.3.1 构造kd树

注意： 这里的k是k维，k近邻算法中的k是最近的k个实例点。

kd树是二叉树，从根节点往下，每一层相当于对k维空间的一个划分partition。

>  kd树的构造过程，就是不断的用垂直于坐标轴的超平面将k维空间划分，构成一系列的k维超矩形区域。其中每个节点对应于一个区域。

构造算法：

> 1. 构造根节点， 对应包含训练集的k维超矩形区域
> 2. 选取第一个分量，对训练集中的数据以第一个分量的中位数为切分点，生成左右子节点，左边小，右边大，同时把落在切分超平面的点保存在根节点
> 3. 重复：依次对其他分量也进行切分，直到两个子区域中没有实例存在。

### 3.3.2 搜索kd树

算法以及例子具体都见原书，比较复杂。

复杂度：

> 如果实例点是随机分布的，kd树搜索的平均计算复杂度为logN，N为训练数据集的大小。

