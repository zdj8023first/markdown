# 第4章 朴素贝叶斯法

朴素贝叶斯 naive Bayes法基于贝叶斯定理与特征条件独立假设的分类方法。

朴素贝叶斯法简述：

> 1. 基于特征条件独立假设学习输入/输出的联合概率分布；
> 2. 基于学习到的模型，对于给定的输入，利用**贝叶斯定理**求出后验概率最大的输出。

特点：

- 实现简单
- 学习和预测的效率都比较高

本章内容：

1. 学习和分类
2. 参数估计算法

内容补充：

> 先验概率：由因求果，如全概率公式
>
> 后验概率：由果索因，如贝叶斯公式

## 4.1 学习与分类

### 4.1.1 基本方法

**学习**：

朴素贝叶斯通过训练数据集学习联合概率分布P( X, Y)。

而联合概率分布是通过学习先验概率分布与条件概率分布，即

> P(Y = ck) 
>
> P(X = x | Y = ck) 

其中对条件概率分布作**条件独立性假设**，这也是朴素贝叶斯的名字由来，条件独立性假设即假设用于分类的特征在类确定的条件是都是条件独立的。这样让学习方法变得简单，但是也以分类准确率为代价。

> 朴素贝叶斯法学习到的是生成数据的机制，所以属于**生成模型**

**分类**：

对于给定的输入x，通过学习到的模型计算后验概率分布P(Y = ck | X =x )， 将后验概率最大的类作为x的类输出。

后验概率的计算由贝叶斯定理计算，即 P(A|B) = P(AB)/P(B)，再结合全概率公式。具体形式见原书P48.

### 4.1.2 后验概率最大化的含义

具体证明请见原书。

简单来说：

> 根据损失函数为0-1损失函数时的期望风险最小化的原则可以得到后验概率最大化准则。

这也是朴素贝叶斯法的原理。

## 4.2 参数估计

### 4.2.1 极大似然估计

由上面知道朴素贝叶斯的学习就是估计先验概率和条件概率。

极大似然估计的形式化定义见原书，简单来讲就是：

> - 先验概率的极大似然估计就是 每一类的数量除以训练数据集的大小；
> - 条件概率的极大似然估计就是 每一类中输入特定值的数量除以这一类的数量

即用频率来估计概率

### 4.2.2 学习分类算法

根据之前的步骤总结朴素贝叶斯算法 naive Bayes algorithm：

输入：训练数据集；实例ｘ

输出：实例x的分类

1. 计算先验概率、条件概率
2. 对于给定的实例x，计算每个类别的后验概率
3. 根据后验概率最大的类别确定输入x的类别



### 4.2.3 贝叶斯估计

极大似然估计有一种特殊情况，即出现分母为0的情况，即所要估计的概率值为0的情况，而贝叶斯估计就是为了解决这一问题而提出的。

形式化定义见原书

形式化定义等价于：

> 在随机变量的各个取值的频数上赋予一个正数a，这样
>
> - 当a = 0 时，就是极大似然估计
> - 当a = 1 时， 称为拉普拉斯平滑Laplace smoothing。

可以验证：

1. P > 0
2. sum(P) =1

证明贝叶斯估计也是一种概率分布。



