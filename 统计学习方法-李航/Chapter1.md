# 第1章 统计学习方法概论

全书的概况，介绍了下整本书的内容，包括

​	统计学习的定义、研究对象、方法；

​	监督学习；

​	统计学习方法：模型、策略、算法；

​	模型选择：正则化、交叉验证、泛化能力；

​	生成模型和判别模型；

​	监督方法的应用： 分类、标注、回归



## 1.1 统计学习

### 1 特点

统计学习 statistical learning 是 计算机 基于 **数据** 构建 **概率统计模型** 然后对数据进行 **预测和分析**的学科。

特点： 

> 计算机以及网络为平台
>
> 数据驱动
>
> 目的是对数据的分析和预测
>
> 以方法为中心，以方法构建模型
>
> 交叉学科，独特理论体系

学习：

> Herbert A Simon: 一个系统通过执行某个过程改进它的性能，就是学习

统计学习--统计机器学习

### 2 对象

数据！ 从数据出发，提取特征，抽象模型，发现知识，分析预测。

统计学习的前提：

> 同类数据具有一定的统计规律性，所以可以用概率统计的方法处理

数据以离散变量为主，不涉及数据的观测和收集



### 3 目的

统计学习对数据进行分析和预测，来提高计算机的智能化，特别指根据已有数据对未知数据

构建概率统计模型，总的目标

> 考虑模型-》学习模型-》提高学习效率

### 4 方法

监督学习、非监督学习、半监督学习、强化学习。

监督学习：

> 训练数据集 training data ,数据独立同分布
>
> 模型属于函数的集合，也就是假设空间 hypothesis space
>
> 评价准则 evaluation criterion。

也就是

> 从 hypothesis space中选取一个最优模型，在评价标准criterion下在 training和test数据上都表现最好

选取过程是由算法来实现的。

这样就得出了统计学习方法的三要素：

> hypothesis space , model
>
> criterion , strategy
>
> algorithm

用来解决：

> 分类、标注、回归等问题

应用：

> 自然语言处理、信息检索、文本数据挖掘

### 5 统计学习的研究

统计学习方法 method、理论 theory、 应用 application

### 6 重要性

人工智能、模式识别、数据挖掘、自然语言处理、语音识别、图像识别、信息检索、生物信息

- 海量数据
- 智能化
- 计算机科学：系统、计算、信息，统计学习属于信息





## 1.2 监督学习

### 1 基本概念

- 输入空间 input space
- 输出空间 output space
- 实例instance： 具体的一个输入，特征向量feature vector
- 特征空间 feature space ，维度-特征，模型都定义在特征空间上，特征空间和输入空间
- 输入变量、输出变量以及它们的值
- training data、 test data
- 样本、样本点 sample

根据输入、输出变量的不同类型，分为

- 回归： 连续-> 连续
- 分类： -> 离散
- 标注：变量序列->变量序列



### ２．联合概率分布

有待复习一下概率的课程中的基本概念

监督学习假设输入和输出的随机变量Ｘ和Ｙ具有联合概率分布，这是基本假设



### 3.假设空间

监督学习：　学习一个由输入到输出的映射，也就是模型。模型也就是映射的集合就是假设空间hypothesis space。可以是概率模型也可以是非概率模型， 概率模型这个同样需要自己再去看下概率论



 ## 2 问题的形式化

监督学习：

> 训练集上学习模型，模型在测试集上进行预测prediction

而训练集和测试集的划分是人工给出的，因此称为监督学习。

> 学习和预测



训练数据和测试数据独立同分布的，假设

条件概率分布和决策函数

> y = max(P(y|x)) or f(x)

通过训练集数据来学习模型，使模型的输出与实际的值越来越小



##  1.3 统计学习的三要素

> 方法 = 模型 + 策略 + 算法

构建一种统计学习方法，就是确定其具体的三要素



### 1.3.1  模型

首要问题就是什么要的模型

对于监督学习：条件概率分布函数或者决策函数

> 假设空间： 决策函数的集合，一般无穷多个

假设空间由参数向量决定的函数簇或者条件概率分布族，参数空间 parameter space取决于n维欧式空间

> 决策函数： 非概率模型
>
> 条件概率分布： 概率模型



### 1.3.2 策略

策略即什么样的学习准则， strategy as criterion

**损失函数和风险函数**

> 损失： 一次预测； 风险： 多次平均



#### 损失函数和风险函数

决策函数f(X),预测值f(X)与真实值Y之间的差异，用损失函数loss function或者代价函数cost function来度量错误程度，非负实数值，记作**L(Y,f(X))**

常见的几种

- 0-1
- 平方 quadratic
- 绝对 absolute
- 对数或者对数似然 log likelihood

由于模型的输入输出遵循联合分布P(X,Y)，所以可以根据积分求出理论上损失函数的期望

也就是L(y, f(x))P(x,y)在x*y上的积分，既是**风险函数risk或者期望损失expected loss**

> 学习目标： 选择期望风险最小的模型

但是这里有一个问题：

> 联合分布P(X,Y)是未知的，所以无法计算风险函数

而且如果联合分布是已知的，就可以直接求出来条件概率分布了，也就***不需要学习了***

矛盾状况如下面描述：

> 学习模型需要用到联合分布，但是联合分布未知，所以监督学习就是个ill-formed problem

经验风险：

> 模型在训练数据集上的平均损失为经验风险 empirical risk 或者经验损失 empirical loss
>
> Remp = 训练集上的损失函数的求和 /  训练数据集的大小

这里面

- 期望风险是模型关于联合分布的期望损失
- 经验风险是模型关于训练数据的期望损失

然后根据大数定理，当样本足够大，就可以无限趋近

而实际情况是训练数据集的大小有限，所以需要对经验风险进行一定的矫正

因此 有两个策略

> 经验风险最小化
>
> 结构风险最小化



#### 2. 经验风险最小化、结构风险最小化

**经验风险最小化**empirical risk minimization ERM： 经验风险最小就是最优的模型！

所以就是一个求解最优化的问题，也就是 min(Remp)

在样本容量足够大的时候，可以保证很好的效果。

例子：

> 极大似然估计： 条件概率分布的模型，标准是对数损失函数时

但是如果样本太小， 容易产生过拟合over-fitting



**结构风险最小化** structural risk minimization 就是为了防止在样本容量过小的情况下避免过拟合的策略

等价于正则化 regularization

>  结构风险最小化 =  经验风险最小化 +  系数J(f)

其中J(f)为模型的复杂度，是假设空间的泛函， 越复杂越大，越简单越小，这样就对复杂模型进行了惩罚

附加的一项 就是正则化项regularizer或者penalty term。系数是非负的！

> 这样只有当经验风险和惩罚项同时小的时候，结构风险才会小

例子

> 贝叶斯估计中的最大后验估计 maximum posterior probability estimation，MAP
>
> 条件概率分布，对数损失函数，模型复杂度由模型的先验概率表示

这样结构风险最小的模型就是最优的模型。因此也是个最优化的问题。



这样监督学习问题就是

> 经验风险或者结构风险函数最小化的问题

对应的相应风险函数也就是最优化的目标函数



### 1.3.3 算法

> 算法指学习模型的具体计算方法

上面得到是函数最优化问题，因此相应的算法也就是求解最优化问题的算法

在没有显式的解析解的时候，可以用数值计算的方法求解



最后统计学习方法之间的不同，主要就在于模型、策略、算法的不同



## 1.4 模型评估和选择

### 1.4.1 训练误差和测试误差

统计学习的目的： 在已知数据和未知数据都有很好的预测能力，也就是训练集和测试集

当损失函数给定的时候，评估标准就是

- 训练误差 training error： 训练集上损失函数的期望
- 测试误差 test error： 测试集上损失函数的期望

这里提到评估标准中的损失函数和统计学习方法具体采用的损失函数不一定一样。当然一样是最好的



这里如果损失函数如果是0-1函数的时候，测试误差就是测试数据集上的**错误率 error rate**, 

相应的也可以定义**准确率accuracy**

相对来说，测试误差比训练误差更重要，因为

> 测试误差反应了学习方法对未知数据的预测能力，这个能力也称为**泛化能力**

相对来说，测试误差小的方法是更有效的方法



### 1.4.2 过拟合 与 模型选择

模型选择 model selection : 当假设空间含有不同复杂度的模型时，就需要进行模型选择

如果假设空间的存在**真模型**的的时候，我们要选择的模型就应该逼近真模型，也就是

> 参数个数相同， 参数向量也相同

**过拟合over-fitting**： 一味的追求对训练数据的预测能力，所选择的复杂度就会比**真模型**更复杂。也即是说

> 过拟合就是学习时选择的模型所包含的参数过多，导致对训练数据预测的很好，但是对未知测试数据很差

模型选择的目的

> 旨在避免过拟合over-fitting以及提高预测能力



例子: M次多项式函数拟合问题，具体详见书籍第 11页，电子书第27页

其中对参数根据偏导数=0求值的过程，需要自己手动计算一下

结论：

> 当M=9时，训练误差为0，但是对未知数据的预测能力往往较差，即过拟合 over-fitting

因此：

> 不仅要考虑训练数据，还要考虑未知数据



学习过程中发现的问题

> 随着模型复杂度的增高， 训练误差会不断减小，直到趋近于0， 而测试误差会先减小后增大。学习的目的是使测试误差达到最小。

这对于其他一般模型也是成立的。

为了达到测试误差最小的目的， 有两种模型选择的方法：正则化和交叉验证



## 1.5 正则化和交叉验证

### 1.5.1 正则化 regularization

如之前所讲的是，正则化是**结构风险最小化**策略的实现，就是在**经验风险**上加上一个正则化项 regularizer或者说penalty term惩罚项。

> 正则化项一般是模型复杂度的单调递增函数，比如模型参数向量的范数

结构化风险：经验风险，正则化项，以及调整这两者关系的系数

**正则项**可以选择参数向量的L1范数和L2范数

这时候

> 如果第一项经验风险较小，说明拟合效果比较好，这时候模型就可能比较复杂（有多个非零参数），
>
> 这样第二项就会比较大，那么正则化后的结构化风险也就比较大了

所以我们要找的就是两项都比较小的模型

[范数](https://zhuanlan.zhihu.com/p/28023308)

[奥卡姆剃刀原理Occam's Razor]([https://baike.baidu.com/item/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80%E5%8E%9F%E7%90%86](https://baike.baidu.com/item/奥卡姆剃刀原理))

> Entities should not be multiplied unnecessarily
>
> 如无必要，勿增实体

在正则化上的应用：

> 能够很好的解释已知数据并且十分简单才是最好的模型

贝叶斯估计

> 正则化项对应于模型的先验概率



### 1.5.2 交叉验证 cross validation

- 样本充足 -> 训练集training set 训练模型， 验证集 validation set 选择模型（不同复杂度）， 测试集 test set 评估模型
- 样本不充足-> 切分，组合成训练集和测试集，重复训练测试以及模型选择

#### 1 简单交叉验证

> 随机分成两部分，如70% training set ， 30% test set
>
> 然后用训练集分别训练不同的模型（如参数个数不同）
>
> 最后在测试集上选择测试误差最小的模型

怎么感觉这个跟经验风险没什么区别啊？？

#### 2 S折（K折）交叉验证 S-fold cross validation

> 随机切分数据为大小相同的S份
>
> S-1份作为训练集，1份作为测试集，循环S次
>
> 取平均测试误差最小的模型

#### 3 留一交叉验证

> 当S=N时，即为留一交叉验证，这里N是给定数据集的容量

这在数据比较缺乏的时候比较有用



## 1.6 泛化能力

泛化能力 generalization ability

> ​	学习方法的泛化能力就是由该学习方法学习到的模型对未知数据的预测能力，这是本质上的重要能力

通常用测试误差来评价，但是测试数据是有限的。

因此需要从理论上分析

### 1.6.1 泛化误差

定义：

> 学习到的模型对未知数据预测的误差

事实上就是前面所提到的**期望风险**



### 1.6.2 上界

泛化误差上届 generalization error bound， 通过它来分析学习方法的泛化能力

性质

- 样本容量的函数，样本容量越来越大，上届逐渐趋近于0
- 假设空间容量的函数，容量越大，模型学习起来越复杂， 上届越大

例子： 二分类问题

详细例子见pdf的33页，书籍的第17页

简单来讲：

> 上界<=经验误差+关于样本容量N、假设空间复杂度d，以及概率的一个函数项

这个函数随着N的增大趋近于0，随着d的增大而增大



## 1.7 生成模型与判别模型

监督学习的任务

> 学习一个模型，应用这个模型预测

模型的形式

> 决策函数或者条件概率函数

监督学习方法

- 生成方法 generative approach - 生成模型 model
- 判别方法 discriminative approach - 判别模型

生成方法：

> 由数据学习联合分布，然后根据联合分布求出条件概率分布作为用来预测的模型

生成模型表示了给定输入产生输出的生成关系

例子：

- 朴素贝叶斯
- 隐马尔科夫链

特点

- 还原出联合概率分布（判别不能）
- 收敛速度快
- 隐变量仍然可以学习（判别不能）

判别方法

> 由数据直接学习决策函数或者条件概率分布作为预测的模型

判别模型是根据输入预测什么样的输出（没太明白）

例子：

- k近邻法
- 感知机
- 决策树
- 逻辑斯蒂回归模型
- 最大熵模型
- 支持向量机
- 提升方法
- 条件随机场

判别方法特点：

- 学习的准确率更高，因为是直接学习用来预测的模型的
- 可以对数据进行抽象，简化学习问题



## 1.8 分类问题

分类问题：

> 当输出变量Y取有限个离散个数时，预测问题就成了分类问题。 输入可连续可离散

分类器classifier ： 分类问题中学习到的模型

预测prediction， 分类 classification， 类 class

- 多类分类
- 二类分类（本书）

分类问题包括**学习**和**分类**，也就是训练和预测的步骤分别

分类问题的工作过程

> ​	学习过程由训练数据学习到一个分类器
>
> ​	分类过程由分类器对新的输入进行分类，预测其输出的类标记

这里感觉分类器应该是判别方法生成的**判别模型**(有待验证)

评价指标

- 准确率 accuracy 损失函数为0-1函数时的测试数据集上的准确率

- 精确率 precision
- 召回率 

根据预测正确不正确，可能出现4种情况

- TP true positive： 正类预测为正类
- FN false negative : 正类预测为负类
- FP false positive ：负类预测为正类
- TN true negative ： 负类预测为负类

精确率定义

> Precision =  TP /  (TP + FP)

召回率定义

> Recall = TP / (TP + FN)

另有它们的调和均值F1

> 2 / F1 = 1 / P + 1 / R

可以用来分类的统计学习方法：

- 基本都可以用。。。



分类的应用

- 银行业务
- 网络安全
- 图像处理
- 手写识别
- 互联网网页分类

应用例子- 文本分类 text classification 



## 1.9 标注问题



