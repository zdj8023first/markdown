# 第1章 统计学习方法概论

全书的概况，介绍了下整本书的内容，包括

​	统计学习的定义、研究对象、方法；

​	监督学习；

​	统计学习方法：模型、策略、算法；

​	模型选择：正则化、交叉验证、泛化能力；

​	生成模型和判别模型；

​	监督方法的应用： 分类、标注、回归



## 1.1 统计学习

### 1 特点

统计学习 statistical learning 是 计算机 基于 **数据** 构建 **概率统计模型** 然后对数据进行 **预测和分析**的学科。

特点： 

> 计算机以及网络为平台
>
> 数据驱动
>
> 目的是对数据的分析和预测
>
> 以方法为中心，以方法构建模型
>
> 交叉学科，独特理论体系

学习：

> Herbert A Simon: 一个系统通过执行某个过程改进它的性能，就是学习

统计学习--统计机器学习

### 2 对象

数据！ 从数据出发，提取特征，抽象模型，发现知识，分析预测。

统计学习的前提：

> 同类数据具有一定的统计规律性，所以可以用概率统计的方法处理

数据以离散变量为主，不涉及数据的观测和收集



### 3 目的

统计学习对数据进行分析和预测，来提高计算机的智能化，特别指根据已有数据对未知数据

构建概率统计模型，总的目标

> 考虑模型-》学习模型-》提高学习效率

### 4 方法

监督学习、非监督学习、半监督学习、强化学习。

监督学习：

> 训练数据集 training data ,数据独立同分布
>
> 模型属于函数的集合，也就是假设空间 hypothesis space
>
> 评价准则 evaluation criterion。

也就是

> 从 hypothesis space中选取一个最优模型，在评价标准criterion下在 training和test数据上都表现最好

选取过程是由算法来实现的。

这样就得出了统计学习方法的三要素：

> hypothesis space , model
>
> criterion , strategy
>
> algorithm

用来解决：

> 分类、标注、回归等问题

应用：

> 自然语言处理、信息检索、文本数据挖掘

### 5 统计学习的研究

统计学习方法 method、理论 theory、 应用 application

### 6 重要性

人工智能、模式识别、数据挖掘、自然语言处理、语音识别、图像识别、信息检索、生物信息

- 海量数据
- 智能化
- 计算机科学：系统、计算、信息，统计学习属于信息





## 1.2 监督学习

### 1 基本概念

- 输入空间 input space
- 输出空间 output space
- 实例instance： 具体的一个输入，特征向量feature vector
- 特征空间 feature space ，维度-特征，模型都定义在特征空间上，特征空间和输入空间
- 输入变量、输出变量以及它们的值
- training data、 test data
- 样本、样本点 sample

根据输入、输出变量的不同类型，分为

- 回归： 连续-> 连续
- 分类： -> 离散
- 标注：变量序列->变量序列



### ２．联合概率分布

有待复习一下概率的课程中的基本概念

监督学习假设输入和输出的随机变量Ｘ和Ｙ具有联合概率分布，这是基本假设



### 3.假设空间

监督学习：　学习一个由输入到输出的映射，也就是模型。模型也就是映射的集合就是假设空间hypothesis space。可以是概率模型也可以是非概率模型， 概率模型这个同样需要自己再去看下概率论



 ## 2 问题的形式化

监督学习：

> 训练集上学习模型，模型在测试集上进行预测prediction

而训练集和测试集的划分是人工给出的，因此称为监督学习。

> 学习和预测



训练数据和测试数据独立同分布的，假设

条件概率分布和决策函数

> y = max(P(y|x)) or f(x)

通过训练集数据来学习模型，使模型的输出与实际的值越来越小



##  1.3 统计学习的三要素

> 方法 = 模型 + 策略 + 算法

构建一种统计学习方法，就是确定其具体的三要素



### 1.3.1  模型

首要问题就是什么要的模型

对于监督学习：条件概率分布函数或者决策函数

> 假设空间： 决策函数的集合，一般无穷多个

假设空间由参数向量决定的函数簇或者条件概率分布族，参数空间 parameter space取决于n维欧式空间

> 决策函数： 非概率模型
>
> 条件概率分布： 概率模型



### 1.3.2 策略

策略即什么样的学习准则， strategy as criterion

**损失函数和风险函数**

> 损失： 一次预测； 风险： 多次平均



#### 损失函数和风险函数

决策函数f(X),预测值f(X)与真实值Y之间的差异，用损失函数loss function或者代价函数cost function来度量错误程度，非负实数值，记作**L(Y,f(X))**

常见的几种

- 0-1
- 平方 quadratic
- 绝对 absolute
- 对数或者对数似然 log likelihood

由于模型的输入输出遵循联合分布P(X,Y)，所以可以根据积分求出理论上损失函数的期望

也就是L(y, f(x))P(x,y)在x*y上的积分，既是**风险函数risk或者期望损失expected loss**

> 学习目标： 选择期望风险最小的模型

但是这里有一个问题：

> 联合分布P(X,Y)是未知的，所以无法计算风险函数

而且如果联合分布是已知的，就可以直接求出来条件概率分布了，也就***不需要学习了***

矛盾状况如下面描述：

> 学习模型需要用到联合分布，但是联合分布未知，所以监督学习就是个ill-formed problem

经验风险：

> 模型在训练数据集上的平均损失为经验风险 empirical risk 或者经验损失 empirical loss
>
> Remp = 训练集上的损失函数的求和 /  训练数据集的大小

这里面

- 期望风险是模型关于联合分布的期望损失
- 经验风险是模型关于训练数据的期望损失

然后根据大数定理，当样本足够大，就可以无限趋近

而实际情况是训练数据集的大小有限，所以需要对经验风险进行一定的矫正

因此 有两个策略

> 经验风险最小化
>
> 结构风险最小化



#### 2. 经验风险最小化、结构风险最小化

**经验风险最小化**empirical risk minimization ERM： 经验风险最小就是最优的模型！

所以就是一个求解最优化的问题，也就是 min(Remp)

在样本容量足够大的时候，可以保证很好的效果。

例子：

> 极大似然估计： 条件概率分布的模型，标准是对数损失函数时

但是如果样本太小， 容易产生过拟合over-fitting



**结构风险最小化** structural risk minimization 就是为了防止在样本容量过小的情况下避免过拟合的策略

等价于正则化 regularization

>  结构风险最小化 =  经验风险最小化 +  系数J(f)

其中J(f)为模型的复杂度，是假设空间的泛函， 越复杂越大，越简单越小，这样就对复杂模型进行了惩罚

附加的一项 就是正则化项regularizer或者penalty term。系数是非负的！

> 这样只有当经验风险和惩罚项同时小的时候，结构风险才会小

例子

> 贝叶斯估计中的最大后验估计 maximum posterior probability estimation，MAP
>
> 条件概率分布，对数损失函数，模型复杂度由模型的先验概率表示

这样结构风险最小的模型就是最优的模型。因此也是个最优化的问题。



这样监督学习问题就是

> 经验风险或者结构风险函数最小化的问题

对应的相应风险函数也就是最优化的目标函数



### 1.3.3 算法

> 算法指学习模型的具体计算方法

上面得到是函数最优化问题，因此相应的算法也就是求解最优化问题的算法

在没有显式的解析解的时候，可以用数值计算的方法求解



最后统计学习方法之间的不同，主要就在于模型、策略、算法的不同



## 1.4 模型评估和选择

### 1.4.1 训练误差和测试误差

统计学习的目的： 在已知数据和未知数据都有很好的预测能力，也就是训练集和测试集

当损失函数给定的时候，评估标准就是

- 训练误差 training error： 训练集上损失函数的期望
- 测试误差 test error： 测试集上损失函数的期望

这里提到评估标准中的损失函数和统计学习方法具体采用的损失函数不一定一样。当然一样是最好的



这里如果损失函数如果是0-1函数的时候，测试误差就是测试数据集上的**错误率 error rate**, 

相应的也可以定义**准确率accuracy**

相对来说，测试误差比训练误差更重要，因为

> 测试误差反应了学习方法对未知数据的预测能力，这个能力也称为**泛化能力**

相对来说，测试误差小的方法是更有效的方法



### 1.4.2 过拟合 与 模型选择

模型选择 model selection : 当假设空间含有不同复杂度的模型时，就需要进行模型选择

如果假设空间的存在**真模型**的的时候，我们要选择的模型就应该逼近真模型，也就是

> 参数个数相同， 参数向量也相同

**过拟合over-fitting**： 一味的追求对训练数据的预测能力，所选择的复杂度就会比**真模型**更复杂。也即是说

> 过拟合就是学习时选择的模型所包含的参数过多，导致对训练数据预测的很好，但是对未知测试数据很差

模型选择的目的

> 旨在避免过拟合over-fitting以及提高预测能力



例子: M次多项式函数拟合问题，具体详见书籍第 11页，电子书第27页

其中对参数根据偏导数=0求值的过程，需要自己手动计算一下

结论：

> 当M=9时，训练误差为0，但是对未知数据的预测能力往往较差，即过拟合 over-fitting

因此：

> 不仅要考虑训练数据，还要考虑未知数据



学习过程中发现的问题

> 随着模型复杂度的增高， 训练误差会不断减小，直到趋近于0， 而测试误差会先减小后增大。学习的目的是使测试误差达到最小。

这对于其他一般模型也是成立的。

为了达到测试误差最小的目的， 有两种模型选择的方法：正则化和交叉验证



## 1.5 正则化和交叉验证

### 1.5.1 正则化 regularization

如之前所讲的是，正则化是**结构风险最小化**策略的实现，就是在**经验风险**上加上一个正则化项 regularizer或者说penalty term惩罚项。

> 正则化项一般是模型复杂度的单调递增函数，比如模型参数向量的范数

结构化风险：经验风险，正则化项，以及调整这两者关系的系数

**正则项**可以选择参数向量的L1范数和L2范数

这时候

> 如果第一项经验风险较小，说明拟合效果比较好，这时候模型就可能比较复杂（有多个非零参数），
>
> 这样第二项就会比较大，那么正则化后的结构化风险也就比较大了

所以我们要找的就是两项都比较小的模型

[范数](https://zhuanlan.zhihu.com/p/28023308)

[奥卡姆剃刀原理Occam's Razor]([https://baike.baidu.com/item/%E5%A5%A5%E5%8D%A1%E5%A7%86%E5%89%83%E5%88%80%E5%8E%9F%E7%90%86](https://baike.baidu.com/item/奥卡姆剃刀原理))

> Entities should not be multiplied unnecessarily
>
> 如无必要，勿增实体

在正则化上的应用：

> 能够很好的解释已知数据并且十分简单才是最好的模型

贝叶斯估计

> 正则化项对应于模型的先验概率



### 1.5.2 交叉验证

