# 第5章 决策树

决策树decision tree是基本的分类和回归的方法，本书主要讨论分类。

决策树模型呈树形结构，表示基于特征对实例进行分类的过程。

主要包括三个步骤：

1. 特征选择
2. 决策树的生成
3. 决策树的修剪

思想来源：

- ID3算法
- C4.5算法
- CART算法



## 5.1 决策树模型与学习

### 5.1.1 模型

决策树由结点node和有向边directed edge组成：

结点分为：

- 内部结点 internal node, 表示一个特征或属性
- 叶节点： 表示一个类

分类流程：

> 1. 从根节点开始，对实例的某一特征进行测试，根据结构分配到相应的子节点，即每个子节点对应该特征的一个取值
> 2. 如此递归的对实例进行测试并分配，直到叶节点的类中

### 5.1.2 if-then 规则

决策树可以看成if-then规则的集合：

> 根节点到叶节点每一条路径对应一条规则，内部结点对应规则的条件，叶节点对应规则的结论。

决策树的路径以及if-then规则都有一个重要的性质

> 互斥且完备



### 5.1.3 条件概率分布

决策树表示给定特征条件下类的条件概率分布。

> 该条件概率分布定义在特征空间的一个划分 partition上；
>
> 将特征空间分为互不相交的cell 或者region；
>
> 在每个单元后者区域上都定义一个类的概率分布就构成了一个条件概率分布。

决策树中的对应关系：

- 一条路径对应划分的一个单元 cell
- 决策树所表示的条件概率分布由各个单元的类的条件概率分布组成

### 5.1.4 决策树学习

学习的目标：

> 根据训练数据集构建一个决策树模型

学习的本质：

> 从训练数据集中归纳出来一组分类规则

同样的，不仅需要与训练数据矛盾小，同时还要有很好的泛化能力。

决策树的学习用损失函数来表示这一目标：

> 损失函数是正则化的极大似然函数，学习策略即损失函数的最小化



因此学习问题 也就是变成了选择最优决策树的问题，这是个NP完全问题，因此采用

> 启发式的方法，近似求解最优化问题，即次最优的sub-optimal



算法简述：

总的来说，算法的过程通常是一个递归地选择最优特征，根据该特征对训练数据进行分割，这个过程也是：

- 对特征空间的划分
- 对决策树的构建

1. 选择一个最优特征，对数据集进行划分
2. 如果子集能够被正确的分类，则把子集分到叶子结点，否则递归的继续1步骤
3. 直到所有子集都能被基本正确分类，或者没有合适的特征了

算法过程可能会发现过拟合，即泛化能力比较弱，因此，需要：

> 对生成的决策树进行自下而上的剪枝，使树的复杂度降低，从而提高数据的泛华能力

具体操作为：

> 对于过于细分的叶节点，使其退回到父节点或者更加往上的节点，作为新的叶节点

如果特征数量即特征空间的维数比较大，则可以首先进行特征选择。



这样也就总结出了决策树学习方法的步骤：

1. 特征选择
2. 决策树的生成
3. 决策树的剪枝

决策树表示一个条件概率分布：

- 树的深度对应模型的复杂度
- 树的生成对应模型的局部选择，考虑局部最优
- 树的剪枝对应模型的全局选择，考虑全局最优



## 5.2 特征选择

### 5.2.1 特征选择问题

特征选择在于选择对数据具有分类能力的特征。

选择的准则是:

> 信息增益或者信息增益比

例子 例5.1贷款申请，详细见原书P58

直观上：

> 如果一个特征具有更好的分类能力，或者说按照该特征使得各个子集有最好的分类，那么就应该选择这个特征

而信息增益information gain能够很好的体现这一点

### 5.2.2 信息增益

信息论和概率统计中，熵entropy：

> 表示随机变量不确定性的度量。

对于取有限个值的离散随机变量X，熵定义为:
$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
由定义可以看到，熵只依赖于随机变量的分布，和随机变量的取值没有关系。









