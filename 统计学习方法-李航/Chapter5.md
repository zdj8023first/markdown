# 第5章 决策树

决策树decision tree是基本的分类和回归的方法，本书主要讨论分类。

决策树模型呈树形结构，表示基于特征对实例进行分类的过程。

主要包括三个步骤：

1. 特征选择
2. 决策树的生成
3. 决策树的修剪

思想来源：

- ID3算法
- C4.5算法
- CART算法



## 5.1 决策树模型与学习

### 5.1.1 模型

决策树由结点node和有向边directed edge组成：

结点分为：

- 内部结点 internal node, 表示一个特征或属性
- 叶节点： 表示一个类

分类流程：

> 1. 从根节点开始，对实例的某一特征进行测试，根据结构分配到相应的子节点，即每个子节点对应该特征的一个取值
> 2. 如此递归的对实例进行测试并分配，直到叶节点的类中

### 5.1.2 if-then 规则

决策树可以看成if-then规则的集合：

> 根节点到叶节点每一条路径对应一条规则，内部结点对应规则的条件，叶节点对应规则的结论。

决策树的路径以及if-then规则都有一个重要的性质

> 互斥且完备



### 5.1.3 条件概率分布

决策树表示给定特征条件下类的条件概率分布。

> 该条件概率分布定义在特征空间的一个划分 partition上；
>
> 将特征空间分为互不相交的cell 或者region；
>
> 在每个单元后者区域上都定义一个类的概率分布就构成了一个条件概率分布。

决策树中的对应关系：

- 一条路径对应划分的一个单元 cell
- 决策树所表示的条件概率分布由各个单元的类的条件概率分布组成

### 5.1.4 决策树学习

学习的目标：

> 根据训练数据集构建一个决策树模型

学习的本质：

> 从训练数据集中归纳出来一组分类规则

同样的，不仅需要与训练数据矛盾小，同时还要有很好的泛化能力。

决策树的学习用损失函数来表示这一目标：

> 损失函数是正则化的极大似然函数，学习策略即损失函数的最小化



因此学习问题 也就是变成了选择最优决策树的问题，这是个NP完全问题，因此采用

> 启发式的方法，近似求解最优化问题，即次最优的sub-optimal



算法简述：

总的来说，算法的过程通常是一个递归地选择最优特征，根据该特征对训练数据进行分割，这个过程也是：

- 对特征空间的划分
- 对决策树的构建

1. 选择一个最优特征，对数据集进行划分
2. 如果子集能够被正确的分类，则把子集分到叶子结点，否则递归的继续1步骤
3. 直到所有子集都能被基本正确分类，或者没有合适的特征了

算法过程可能会发现过拟合，即泛化能力比较弱，因此，需要：

> 对生成的决策树进行自下而上的剪枝，使树的复杂度降低，从而提高数据的泛华能力

具体操作为：

> 对于过于细分的叶节点，使其退回到父节点或者更加往上的节点，作为新的叶节点

如果特征数量即特征空间的维数比较大，则可以首先进行特征选择。



这样也就总结出了决策树学习方法的步骤：

1. 特征选择
2. 决策树的生成
3. 决策树的剪枝

决策树表示一个条件概率分布：

- 树的深度对应模型的复杂度
- 树的生成对应模型的局部选择，考虑局部最优
- 树的剪枝对应模型的全局选择，考虑全局最优



## 5.2 特征选择

### 5.2.1 特征选择问题

特征选择在于选择对数据具有分类能力的特征。

选择的准则是:

> 信息增益或者信息增益比

例子 例5.1贷款申请，详细见原书P58

直观上：

> 如果一个特征具有更好的分类能力，或者说按照该特征使得各个子集有最好的分类，那么就应该选择这个特征

而信息增益information gain能够很好的体现这一点

### 5.2.2 信息增益

信息论和概率统计中，熵entropy：

> 表示随机变量不确定性的度量。

对于取有限个值的离散随机变量X，熵定义为:
$$
H(p)=-\sum_{i=1}^{n} p_{i} \log p_{i}
$$
由定义可以看到，熵只依赖于随机变量的分布，和随机变量的取值没有关系。

易得：

> 0 <= H(p) <= logn

且 熵越大，随机变量的不确定性越大。

对于随机变量(X, Y)的联合概率分布，X给定下Y的条件概率分布的条件熵conditional entropy对X的数学期望为：
$$
H(Y | X)=\sum_{i=1}^{n} p_{i} H\left(Y | X=x_{i}\right)
$$
当通过训练数据中的数据估计得到熵和条件熵的时候，分别称为经验熵empirical entropy和经验条件熵empirical conditional entropy。

所以信息增益：

> 表示得知特征X的信息而使得类Y的信息不确定性减少的程度。

信息增益的更夹详尽的定义：

特征A对训练数据集D的信息增益information gain， g(D, A), 定义为：

> 集合D的经验熵H(D)与特征A给定情况下D的经验条件熵H(D|A)之差，即：
>
> g(D,A) = H(D) - H(D|A)

互信息mutual information定义为：

> 对于条件变量X，Y， 熵H(Y)与条件熵H(Y|X)之差

**在决策树学习中信息增益information gain等价于训练数据集中类与特征的互信息**



总得来说：信息增益大的特征具有更强的分类能力。

所以由信息增益准则的特征选择方法为：

> 对于训练数据集D，计算每个特征的信息增益，选择最大的那个

信息增益的算法：

1. 计算数据集D的经验熵
2. 计算特征A对D的经验条件熵
3. 计算两者的差

详细数学表达式请见原书P62

### 5.2.3 信息增益比

由上述定义，信息增益值的大小是差值，所以训练数据集的经验熵越大，信息增益越大，并没有绝对意义

因此使用信息增益比information gain ratio：

> 信息增益与训练数据集D的经验熵之比



## 5.3 决策树的生成

决策树的生成算法： ID3以及C4.5生成算法。



### 5.3.1 ID3算法

**ID3**算法（Iterative Dichotomiser 3 迭代二叉树3代）是一个由[Ross Quinlan](https://zh.wikipedia.org/w/index.php?title=Ross_Quinlan&action=edit&redlink=1)发明的用于[决策树](https://zh.wikipedia.org/wiki/决策树)的[算法](https://zh.wikipedia.org/wiki/算法)

算法核心：

> 在决策树的各个节点应用**信息增益准则**选择特征，递归的构建。即：
>
> 1. 从根节点root node开始，计算所有特征的信息增益，选择最大的作为节点的特征；
> 2. 由该特征建立子节点；
> 3. 对子节点递归地调用以上方法，建立决策树。
> 4. 知道信息增益都很小或者没有特征

该算法相当于用**极大似然估计法**进行概率模型的选择。

算法的详细步骤：

输入：训练数据级D，特征集A，阈值a

输入：决策树T

1. 如果D只有一类，则T为单节点树，就这一类，返回
2. 如果A为空集（？），也为单节点树，则把实例数最大的类作为类标记，返回
3. 否则，按照计算方法得到各个特征的信息增益，选择最大的的特征Ag
4. 如果信息增益值小于阈值，则还是单节点树，同2，返回
5. 否则，按照Ag的可能取值构建子节点，划分训练数据集成各个子集，并把各个子集中实例数最大的类作为类标记。然后返回根节点和子节点构成的决策树
6. 对于子节点，递归地调用该算法。



特点：只有决策树的生成，所以容易产生过拟合。



### 5.3.2 C4.5算法

与ID3算法类似，但是在选择特征的时候，选择信息增益比，而不是信息增益。（有区别么）



## 5.4 剪枝

上述算法生成决策树对训练数据分类很准确，对未知数据效果却不好，即过拟合，过拟合over fitting 的原因在于：

> 在学习时过多的考虑如何提高对训练数据的正确分类，从而构建出**过于复杂**的模型。

解决方法即： 考虑决策树的复杂度，对其进行简化，即剪枝pruning：

> 将生成的树进行简化，具体地：从生成的树剪掉一些子树或叶节点，并将子树的根节点或者相应的父节点作为新的叶节点。



决策树的剪枝通过极小化决策树整体的损失函数loss function或者代价函数cost function来实现。

具体形式请见原书P66.

简要来说：

>  通过将损失函数loss function或者代价函数cost function进行极小化。
>
> 与前面章节中的介绍的损失函数的正则化类似，加入一个用来调整训练数据的经验风险和模型复杂度之间关系的惩罚项。



剪枝，即当两者之间的系数确定是，损失函数最小的模型。

生成和剪枝的关系：

> - 决策树的生成只考虑提高信息增益对训练数据进行更好的拟合，学习局部的模型；
> - 决策树的剪枝通过优化损失函数，考虑减小模型的复杂度， 学习整体的模型。

损失函数的极小化，等价于正则化的极大似然估计。

剪枝算法的步骤：

输入：生成算法产生的决策树T，参数a

输出：修剪后的子树

1. 计算每个节点的经验熵
2. 递归地从树的叶子节点向上收缩：如果叶节点回缩到父节点之后，对应的损失函数减小，则进行剪枝
3. 返回2，直到不能继续，得到损失函数最小的决策树



由算法可知，每一次只考虑两个数的损失函数之差，只在局部进行，可以用动态规划的算法实现。



## 5.5  CART 算法

CART 算法： 分类与回归树算法 classification and regression tree：

> CART算法是在给定输入X下输出Y的条件概率分布的学习方法。（二叉树）

1. 训练数据集生成决策树，尽可能大(拟合训练数据)
2. 剪枝，验证数据集进行剪枝，用损失函数作为标准。



### 5.5.1 生成

决策树的生成就是递归地构建二叉决策树的过程。

- 对回归树， 基于平方误差最小化
- 对分类树， 基于基尼指数Gini Index最小化准则



#### 5.5.1.1 回归树的生成

X，Y为输入输出变量，其中Y为连续变量。

一个回归树对应于：

> 对输入空间（特征空间）的一个划分以及在划分单元上的输出值。

对于划分的M个单元以及每个单元上面的固定值cm，回归树模型可以表示为：
$$
f(x)=\sum_{m=1}^{M} c_{m} I\left(x \in R_{m}\right)
$$
用平方误差表示回归树的预测误差，即平方误差最小来确定最优输出值:

> 可以证明，最优输出值为所有输入实例的平均值



通过启发式的方法确定对输入空间的划分：递归的确定切分变量splitting variable和切分点splitting point。

具体的，计算：
$$
\min _{j, s}\left[\min _{c_{1}} \sum_{x_{i} \in R_{1}(j ; s)}\left(y_{i}-c_{1}\right)^{2}+\min _{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}\right]
$$
如此对每个区域重复此过程。称为最小二乘回归树 least squares regression tree

算法总结步骤如下：

输入： 训练数据集

输出： 回归树f(x)

1. 选择最优切分变量及切分点对
2. 用选择好的切分对划分区域并决定相应的输出值。
3. 继续对两个子区域调用1,2
4. 将输入空间划分为M个区域，生成决策树



#### 5.5.1.2 分类树的生成

分类树基于**基尼指数**选择最优特征，同时决定最优切分点。

分类问题中，基尼指数：

对于K个类，输入每一类的概率分别是pk，则概率分布的基尼指数Gini Index,为：
$$
\operatorname{Gini}(p)=\sum_{k=1}^{K} p_{k}\left(1-p_{k}\right)=1-\sum_{k=1}^{K} p_{k}^{2}
$$
给定样本集，用频率代替概率即可。

而根据特征A将样本集分为两部分之后，则在A的条件下，集合的基尼指数定义为：

<center>G(D,A) = D1/D *G(D1) + D2/D * G(D2) </center>

表示经A分割后D的不确定性，值越大，不确定性越大。

为何基尼指数和熵能够表示数据集的不确定性，简单的例子可以见原书P70，图5.7.：

> 在二分类的情况下，熵之半、基尼指数，以及误分类率三条曲线非常接近。

算法5.6 （CART生成算法）

输入： 训练数据集， 停止条件

输出： 决策树

从根节点，递归操作：

1. 对该节点的训练数据集D，计算现有特征对该数据集的基尼基数，对每个特征的每个取值，计算A为相应值时的基尼基数
2. 选取基尼基数最小的值作为最优特征以及最优切分点，划分数据集。
3. 对子节点递归调用，直到停止条件
4. 生成CART决策树

停止条件：

- 样本树小于特定阈值

- 基尼指数小于特定阈值
- 没有更多特征

### 5.5.2 剪枝

剪枝为了提高泛化能力，避免过拟合，主要有两步：

1. 从决策树底端，开始不断剪枝，直到根节点，从而行程一个子树序列
2. 通过交叉验证在独立的验证数据集对子树序列进行测试。



#### 5.5.2.1 剪枝形成子序列

剪枝过程，计算子树的损失函数：

<center>Ca(T) = C(T) + a|T| </center>

同样的，a用来权衡拟合度和模型的复杂度（泛化能力）

损失函数最小的意义即是最优的。

可以通过**递归**的方法对树进行剪枝：

> 将a从小增大，得到相应区间的a的情况下的子树序列。（不是很理解）

具体过程见原书P73

#### 5.5.2.2 交叉验证

利用独立的验证数据集，测试子树序列中各个子树的平均误差或者基尼指数。最小的即可认为是最优的决策树，并且可以得到最优的正则化系数a



剪枝算法总结如下：

输入：生成的决策树

输出： 剪枝后的最优决策树

1. 设置处置T0，a=正无穷
2. 从下而上对内部节点t计算**剪枝后整体损失函数减少的程度**：g(t),并更新a为最小的g(t)
3. 自上而下的访问内部节点，如果g(t) = a ， 则进行剪枝，并对新的叶节点t以多数表决更新其类。从而等到一个新的子树
4. 递归的重复剪枝过程。
5. 利用交叉验证选择最优子树

